{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0So4QvATZMA"
   },
   "source": [
    "# **iMaterialist Challenge (Furniture) at FGVC5**\n",
    "## **TFNW Kaggle Team**\n",
    "\n",
    "### **Get Started**\n",
    "\n",
    "To get started, you will:\n",
    "\n",
    "\n",
    "\n",
    "1.   Join the Completion\n",
    "2.   Create a Kaggle API key\n",
    "3.   Install the Kaggle Python module\n",
    "4.   Download the dataset for the competition\n",
    "\n",
    "**Join The Completion**\n",
    "\n",
    "Goto Kaggle.com and login.\n",
    "\n",
    "Once logged in, goto the competition page and select the JOIN COMPETITION button\n",
    "\n",
    "https://www.kaggle.com/c/imaterialist-challenge-furniture-2018\n",
    "\n",
    "**Create Kaggle API Key**\n",
    "\n",
    "To download this dataset you will use the Kaggle API for downloading datasets. To use the API, you must first create a Kaggle API Key. To create your API Key, do:\n",
    "\n",
    "\n",
    "\n",
    "1.   Click on your Profile\n",
    "2.   Select My Accont\n",
    "3.   Under API, Select Create New API Token\n",
    "4.   Download the API Key (kaggle.json)\n",
    "5.   Copy the API key to:  ~/.kaggle/kaggle.json\n",
    "6.   On Windows, that would be: \\Users\\<username>\\.kaggle\\kaggle.json\n",
    "\n",
    "**Install the Kaggle Python Module**\n",
    "\n",
    "C:> pip install kaggle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: kaggle in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n"
     ]
    }
   ],
   "source": [
    "# Get the latest version of pip\n",
    "!python -m pip install --upgrade pip\n",
    "\n",
    "# Install the Kaggle module\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_qwEEOlZEKi"
   },
   "source": [
    "\n",
    "\n",
    "### Download the Dataset Dictionary\n",
    "\n",
    "The dataset consists of a dataset dictionary and the data. The dataset dictionary is a JSON file that contains the URL location and label for each image in the dataset. We need to download this first using the Kaggle python module:\n",
    "\n",
    "C:> kaggle competitions download -c imaterialist-challenge-furniture-2018\n",
    "\n",
    "This will place the data under:\n",
    "\n",
    "Linux/Mac: ~/.Kaggle/competitions/imaterialist-challenge-furniture-2018<br/>\n",
    "Windows: \\Users\\<username>\\.Kaggle\\competitions\\imaterialist-challenge-furniture-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json: Skipping, found more recently modified local copy (use --force to force download)\n",
      "validation.json: Skipping, found more recently modified local copy (use --force to force download)\n",
      "test.json: Skipping, found more recently modified local copy (use --force to force download)\n",
      "sample_submission_randomlabel.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c imaterialist-challenge-furniture-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will install and import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from h5py)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from h5py)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n"
     ]
    }
   ],
   "source": [
    "# numpy for the high performance in-memory matrix/array storage and operations.\n",
    "!pip install numpy   \n",
    "# h5py for the HD5 filesystem high performance file storage of big data.\n",
    "!pip install h5py   \n",
    "# Python image manipulation library (replaces PIL)\n",
    "!pip install pillow  \n",
    "# requests for HTTP operations\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy for the high performance in-memory matrix/array storage and operations.\n",
    "import numpy as np\n",
    "\n",
    "# Import h5py for the HD5 filesystem high performance file storage of big data.\n",
    "import h5py\n",
    "\n",
    "# Import PIL.Image for Python image manipulation library. \n",
    "from PIL import Image\n",
    "\n",
    "# Import json and requests for HTTP operations\n",
    "import json, requests\n",
    "\n",
    "# Import the Byte and String IO library for extracing data returned (response) frome HTTP requests.\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "# Import time to record timing\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUaKvpVUbxIm"
   },
   "source": [
    "# **Loading the Dataset (ETL)**\n",
    "\n",
    "## ** Overview **\n",
    "\n",
    "The loading and storing of the image data and corresponding data is done in several steps. To facilate minimizing the loading time and memory requirements, the process is broken into batches, and loaded and stored in concurrent (parallel) groups. \n",
    "\n",
    "Both the size of batches and the number of concurrent load/stores of batches is configurable. The overall process is as follows:\n",
    "\n",
    "1. Load the Dataset Dictionary (described below) for the training (or test or validation) data.\n",
    "2. Determine the number of images to load/store from the data dictionary.\n",
    "3. Split the loading/storing of images into batches, based on batch size.\n",
    "4. Sequentially load groups of batches, but where each batch within the batch group is concurrently processed (loaded and stored).\n",
    "\n",
    "## ** Dispatch **\n",
    "\n",
    "The function load_dispatcher() handles the dispatching of loading/storing of batches. It starts by\n",
    "taking the location of the dataset dictionary and loads it into memory. The dataset dictionary contains the location of each image and corresponding label.\n",
    "\n",
    "The dataset dictionary is in json format, as follows, where [image] is a list of image locations and [annotations] is a list of corresponding labels.\n",
    " \n",
    " {<br/>\n",
    "\"images\" : [image],<br/>\n",
    "\"annotations\" : [annotation],<br/>\n",
    "}\n",
    "\n",
    "The dispatcher makes a HTTP request (requests.get(url)) for the dataset dictionary and extracts the dataset dictionary from the contents of the response (requests.get(url).content). The dispatcher extracts it as raw byte data (ByteIO) and loads the dataset dictionary into a json format (json.load).\n",
    " \n",
    "The dispatcher calculates the number of batches based on the number of images and the batch size. The batches are then sequenced into groups (for i in range(0, batches, concurrent)), where the size of the group is the number of concurrent threads. For each group, the dispatcher creates a thread for each batch to load/store asynchronously (i.e., job). The dispatcher waits for all the concurrent jobs to complete. Based on the time to process the batch group, the dispatcher estimates the time to load/store all the remaining batches.\n",
    "\n",
    "Note, you can set the size of the batches, the size to rescale the images to, and whether to convert to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for thread execution\n",
    "import threading\n",
    "\n",
    "def load_dispatcher(url, batch_type, batch_size=200, size=(300,300), grayscale=False, normalize=False, concurrent=5):\n",
    "    \"\"\" Load the Data in Batches \n",
    "    url - location of data dictionary\n",
    "    batch_type - training, validation or test\n",
    "    batch_size - size of the batch\n",
    "    size - size to rescale image to\n",
    "    grayscale - flag to convert image to grayscale\n",
    "    concurrent - the number of concurrent (parallel)) batch loads\n",
    "    \"\"\"\n",
    "    \n",
    "    # First retreive the dataset dictionary, which is in a JSON format. \n",
    "    # Dictionary is stored remote: We will make a HTTP request\n",
    "    if url.startswith(\"http\"):\n",
    "        datadict = json.load( requests.get(url).content )\n",
    "    # Dictionary is stored locally\n",
    "    else:\n",
    "        datadict = json.load( open( url ) )\n",
    "   \n",
    "    # The number of batches\n",
    "    batches = int(len(datadict['images']) / batch_size)\n",
    "    \n",
    "    # Sequentially Load each batch group (i.e., concurrent)\n",
    "    for i in range(0, batches, concurrent):\n",
    "        # Start time for the batch group\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Listof threads, corresponding to to the processing of each batch in the batch group\n",
    "        threads = []\n",
    "        # Create and Start a processing thread for each batch in the batch group\n",
    "        for j in range(concurrent):\n",
    "            t = threading.Thread(target=load_and_store_batch, args=(datadict, batch_type, i + j, batch_size, size, grayscale, normalize, ))\n",
    "            # Keep track (remember) of the thread\n",
    "            threads.append(t)\n",
    "            # Start the thread\n",
    "            t.start()\n",
    "        # Join the threads into a single wait for all threads to complete\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "                  \n",
    "        # Calculate elapsed time in seconds to load this batch group\n",
    "        elapse = int(time.time() - start_time)\n",
    "            \n",
    "        # Estimate remaining time in minutes for loading remaining barches.\n",
    "        remaining = int( ( ( batches - i ) / concurrent ) * elapse ) / 60\n",
    "        \n",
    "        print(\"Remaining time %d mins\" % remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Load and Store the Data Batches **\n",
    "\n",
    "We have a mass amount of images: 194,828 training, 6,400 validation, 12,800 test. If we tried to load it all, we would need 54GB of memory! \n",
    "\n",
    "Instead, we will load the data into smaller training (validation and test) batches and store them separately on disk. We use the loaded data dictionary and sequentially move through it (batch_size at a time) to build the batches and save them into a HD5 high performance file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_store_batch(datadict, batch_type, pos, batch_size, size, grayscale, normalize):\n",
    "    \"\"\" Process loading (extration), handling (transformation) and storing (loading) as a batch \n",
    "    batch_type - training, validation or test\n",
    "    pos - the batch slice position in the data (i.e., the first, the second, etc)\n",
    "    batch_size - size of the batch\n",
    "    size - size to rescale image to\n",
    "    grayscale - flag to convert image to grayscale\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    start = pos * batch_size\n",
    "    images, labels = load_batch(datadict, start, batch_size, size, grayscale, normalize )\n",
    "        \n",
    "    # Calculate elapsed time in seconds to load this batch\n",
    "    elapse = int(time.time() - start_time)\n",
    "        \n",
    "    print(\"Batch Loaded %d: %d secs\" % (pos, elapse))\n",
    "        \n",
    "    # Write the batch to disk as HD5 file\n",
    "    with h5py.File('contents\\\\' + batch_type + '\\\\images' + str(pos) + '.h5', 'w') as hf:\n",
    "        hf.create_dataset(\"images\",  data=images)\n",
    "    #with h5py.File('contents\\\\' + batch_type + '\\\\labels' + str(pos) +  '.h5', 'w') as hf:\n",
    "        hf.create_dataset(\"labels\",  data=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Extraction / Dataset Directory **\n",
    "\n",
    "The load_batch() function uses the location of each image specified in the slice of the dataset dictionary to load the images into memory, do transformations and store the images on disk, as a batch. The slice is defined by a start position in the dataset dictionary and length, denoted by batch_size.\n",
    "\n",
    "## **Transform / Load**\n",
    " \n",
    "The images are a mix of grayscale, RGB and RGBA (alpha channel) images, and are of different pixel sizes. For the neural network, they all need to be the same size. We will rescale each of our images to be 300 by 300 pixels (default), but you can choose another scale with the size parameter. THe image data will then be packed into a high performance numpy 3D matrix. The row/column are the height and width (300,300) and the third dimension are the channels (3). \n",
    "\n",
    "If the parameter 'grayscale' is True, all the images are converted to grayscale (single channel); otherwise all the grayscale and RGBA images, are converted to RGB.\n",
    "\n",
    "If the parameter 'normalize' is True, all the pixel values are converted from 0 .. 255 (int) to 0 .. 1 (float). *Warning* - non-normalized, the pixels are stored as 8bit integers. If normalized, they are stored as 64bit floating point, and the file will be 8 times as big.\n",
    "\n",
    "The images will be stored in the list images[] and the corresponding labels in the list labels[].\n",
    "\n",
    "The load_batch() function loads a batch of images from the training, validation or test data and does the transform function. The images and corresponding labels are then returned as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n_j3e0kiZELK"
   },
   "outputs": [],
   "source": [
    "timeout = 6   # timeout (seconds) for reading the image from the web\n",
    "retries = 2   # Number of times to retry reading the image over the network\n",
    "\n",
    "def load_batch(datadict, start, batch_size, size, grayscale, normalize):\n",
    "    \"\"\" Load the training datas\n",
    "    datadict - data image/label dictionary\n",
    "    start - index to start reading batch of images\n",
    "    batch_size - number of images to read (None = all images)\n",
    "    grayscale - flag if image should be converted to grayscale\n",
    "    \"\"\"\n",
    "    \n",
    "    images = [] # List containing the images\n",
    "    labels = [] # List containing the corresponding labels for the images\n",
    "    \n",
    "    # Number of images to load\n",
    "    if batch_size == None:\n",
    "        batch_size = len(datadict['images'])\n",
    "      \n",
    "    # Final shape of image Height, Width\n",
    "    if grayscale == True:\n",
    "        shape = size\n",
    "    # Final shape of image Height, Width, Channels(3)\n",
    "    else:\n",
    "        shape = size + (3,)\n",
    "        \n",
    "    not_loaded = 0 # Number of images that failed to load in the batch\n",
    "            \n",
    "    # Load the batch of images/labels from the Data Dictionary\n",
    "    end = start + batch_size\n",
    "    for i in range(start, end): \n",
    "        image_url = datadict['images'][i]['url'][0]\n",
    "        label_id  = datadict['annotations'][i]['label_id']\n",
    "\n",
    "        # Keep trying to read the image over the network on failure upto retries number of times\n",
    "        for retry in range(retries):\n",
    "            # Download, resize and convert images to arrays\n",
    "            try:\n",
    "                # Make HTTP request fot the image data\n",
    "                response = requests.get(image_url, timeout=10)\n",
    "\n",
    "                # Use the PIL.Image libary to load the image data as au uncompressed RGB or Grayscale bitmap\n",
    "                if grayscale == True:\n",
    "                    pixels = Image.open(BytesIO(response.content)).convert('LA')\n",
    "                else:\n",
    "                    pixels = Image.open(BytesIO(response.content))\n",
    "\n",
    "                # Resize the image to be all the same size\n",
    "                pixels = pixels.resize(size, resample=Image.LANCZOS)\n",
    "\n",
    "                # Load the image into a 3D numpy array\n",
    "                image = np.asarray(pixels)\n",
    "\n",
    "                # Discard image if it does not fit the final shape\n",
    "                if image.shape != shape:\n",
    "                    if grayscale == False:\n",
    "                        # Was a gray scale image\n",
    "                        if image.shape == size:\n",
    "                            # Extend to three channels, replicating the single channel\n",
    "                            pixels = pixels.convert('RGB')\n",
    "                            image = np.asarray(pixels)\n",
    "                            break\n",
    "                        # Is RGBA image (4 channels)\n",
    "                        if image.shape == size + (4,):\n",
    "                            # Remove Alpha Channel from Image\n",
    "                            pixels = pixels.convert('RGB')\n",
    "                            image = np.asarray(pixels)\n",
    "                            break\n",
    "                            \n",
    "                    # Unrecognized shape\n",
    "                    not_loaded += 1\n",
    "                    retry = retries\n",
    "                    break\n",
    "            except Exception as ex:\n",
    "                if retry < retries-1:\n",
    "                    continue\n",
    "                #print(\"CAN'T FETCH IMAGE\", image_url)\n",
    "                retry = retries\n",
    "            # Image was read or failed retries number of times\n",
    "            break\n",
    "                \n",
    "        if retry == retries:\n",
    "            not_loaded += 1\n",
    "            continue\n",
    "\n",
    "        # if bad image, skip\n",
    "        if np.any(image == None):\n",
    "            continue\n",
    "            \n",
    "        # Normalize the image (convert pixel values from int range 0 .. 255 to float range 0 .. 1)\n",
    "        if normalize == True:\n",
    "            image = image / 255\n",
    "            \n",
    "        # add image to images list\n",
    "        images.append( image )\n",
    "        # add corresponding label to labels list\n",
    "        labels.append( label_id )\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            print('%d Images added, %d not loaded' % ((i + 1), not_loaded))\n",
    "\n",
    "    return images, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Load\n",
    "\n",
    "The URLs below are for more laptop. You will need to modify it to the location on your laptop.\n",
    "\n",
    "Running as 5 concurrent processes in batches of 200, takes 1/2 day on my laptop with my local Internet service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KqrObUaMZELc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file contents already exists.\n",
      "A subdirectory or file contents\\\\train already exists.\n",
      "A subdirectory or file contents\\\\validation already exists.\n",
      "A subdirectory or file contents\\\\test already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 Images added, 1 not loaded\n",
      "450 Images added, 1 not loaded\n",
      "50 Images added, 1 not loaded\n",
      "250 Images added, 2 not loaded\n",
      "650 Images added, 2 not loaded\n",
      "100 Images added, 1 not loaded\n",
      "900 Images added, 3 not loaded\n",
      "500 Images added, 2 not loaded\n",
      "950 Images added, 3 not loaded\n",
      "700 Images added, 4 not loaded\n",
      "150 Images added, 1 not loaded\n",
      "550 Images added, 2 not loaded\n",
      "300 Images added, 4 not loaded\n",
      "200 Images added, 1 not loaded\n",
      "Batch Loaded 0: 228 secs\n",
      "350 Images added, 5 not loaded\n",
      "600 Images added, 4 not loaded\n",
      "Batch Loaded 2: 272 secs\n",
      "750 Images added, 6 not loaded\n",
      "800 Images added, 9 not loaded\n",
      "Batch Loaded 3: 444 secs\n",
      "1000 Images added, 6 not loaded\n",
      "Batch Loaded 4: 448 secs\n",
      "400 Images added, 8 not loaded\n",
      "Batch Loaded 1: 542 secs\n",
      "Remaining time 1759 mins\n",
      "1250 Images added, 0 not loaded\n",
      "1450 Images added, 1 not loaded\n",
      "1650 Images added, 3 not loaded\n",
      "1050 Images added, 2 not loaded\n",
      "1850 Images added, 1 not loaded\n",
      "1300 Images added, 0 not loaded\n",
      "1500 Images added, 1 not loaded\n",
      "1100 Images added, 4 not loaded\n",
      "1550 Images added, 1 not loaded\n",
      "1700 Images added, 4 not loaded\n",
      "1900 Images added, 2 not loaded\n",
      "1350 Images added, 2 not loaded\n",
      "1600 Images added, 1 not loaded\n",
      "Batch Loaded 7: 180 secs\n",
      "1750 Images added, 4 not loaded\n",
      "1150 Images added, 6 not loaded\n",
      "1950 Images added, 3 not loaded\n",
      "1800 Images added, 5 not loaded\n",
      "Batch Loaded 8: 266 secs\n",
      "1400 Images added, 3 not loaded\n",
      "Batch Loaded 6: 291 secs\n",
      "1200 Images added, 8 not loaded\n",
      "Batch Loaded 5: 374 secs\n",
      "2000 Images added, 5 not loaded\n",
      "Batch Loaded 9: 378 secs\n",
      "Remaining time 1224 mins\n",
      "2850 Images added, 0 not loaded\n",
      "2450 Images added, 0 not loaded\n",
      "2050 Images added, 2 not loaded\n",
      "2900 Images added, 0 not loaded\n",
      "2500 Images added, 1 not loaded\n",
      "2650 Images added, 0 not loaded\n",
      "2250 Images added, 0 not loaded\n",
      "2950 Images added, 0 not loaded\n",
      "2550 Images added, 1 not loaded\n",
      "2100 Images added, 2 not loaded\n",
      "2300 Images added, 0 not loaded\n",
      "2700 Images added, 2 not loaded\n",
      "3000 Images added, 3 not loaded\n",
      "Batch Loaded 14: 234 secs\n",
      "2600 Images added, 2 not loaded\n",
      "Batch Loaded 12: 252 secs\n",
      "2150 Images added, 2 not loaded\n",
      "2350 Images added, 2 not loaded\n",
      "2750 Images added, 2 not loaded\n",
      "2200 Images added, 2 not loaded\n",
      "Batch Loaded 10: 345 secs\n",
      "2400 Images added, 3 not loaded\n",
      "Batch Loaded 11: 346 secs\n",
      "2800 Images added, 3 not loaded\n",
      "Batch Loaded 13: 347 secs\n",
      "Remaining time 1115 mins\n",
      "3650 Images added, 0 not loaded\n",
      "3450 Images added, 3 not loaded\n",
      "3700 Images added, 1 not loaded\n",
      "3500 Images added, 3 not loaded\n",
      "3250 Images added, 1 not loaded\n",
      "3750 Images added, 2 not loaded\n",
      "3050 Images added, 2 not loaded\n",
      "3550 Images added, 4 not loaded\n",
      "3100 Images added, 2 not loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\PIL\\Image.py:918: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3850 Images added, 4 not loaded\n",
      "3600 Images added, 5 not loaded\n",
      "Batch Loaded 17: 302 secs\n",
      "3150 Images added, 2 not loaded\n",
      "3300 Images added, 5 not loaded\n",
      "3800 Images added, 3 not loaded\n",
      "Batch Loaded 18: 364 secs\n",
      "3900 Images added, 4 not loaded\n",
      "3200 Images added, 3 not loaded\n",
      "Batch Loaded 15: 383 secs\n",
      "3350 Images added, 6 not loaded\n",
      "3950 Images added, 4 not loaded\n",
      "3400 Images added, 6 not loaded\n",
      "Batch Loaded 16: 440 secs\n",
      "4000 Images added, 4 not loaded\n",
      "Batch Loaded 19: 472 secs\n",
      "Remaining time 1508 mins\n",
      "4650 Images added, 0 not loaded\n",
      "4250 Images added, 0 not loaded\n",
      "4450 Images added, 1 not loaded\n",
      "4050 Images added, 0 not loaded\n",
      "4850 Images added, 4 not loaded\n",
      "4700 Images added, 1 not loaded\n",
      "4100 Images added, 0 not loaded\n",
      "4500 Images added, 2 not loaded\n",
      "4750 Images added, 2 not loaded\n",
      "4900 Images added, 9 not loaded\n",
      "4150 Images added, 0 not loaded\n",
      "4550 Images added, 5 not loaded\n",
      "4300 Images added, 1 not loaded\n",
      "4800 Images added, 4 not loaded\n",
      "Batch Loaded 23: 314 secs\n",
      "4600 Images added, 7 not loaded\n",
      "Batch Loaded 22: 317 secs\n",
      "4200 Images added, 0 not loaded\n",
      "Batch Loaded 20: 328 secs\n",
      "4350 Images added, 2 not loaded\n",
      "4950 Images added, 11 not loaded\n",
      "4400 Images added, 3 not loaded\n",
      "Batch Loaded 21: 391 secs\n",
      "5000 Images added, 15 not loaded\n",
      "Batch Loaded 24: 479 secs\n",
      "Remaining time 1523 mins\n",
      "5050 Images added, 2 not loaded\n",
      "5650 Images added, 1 not loaded\n",
      "5450 Images added, 2 not loaded\n",
      "5250 Images added, 2 not loaded\n",
      "5900 Images added, 1 not loaded\n",
      "5100 Images added, 5 not loaded\n",
      "5500 Images added, 3 not loaded\n",
      "5300 Images added, 2 not loaded\n",
      "5700 Images added, 3 not loaded\n",
      "5150 Images added, 7 not loaded\n",
      "5950 Images added, 2 not loaded\n",
      "5550 Images added, 6 not loaded\n",
      "5750 Images added, 5 not loaded\n",
      "6000 Images added, 2 not loaded\n",
      "Batch Loaded 29: 402 secs\n",
      "5350 Images added, 3 not loaded\n",
      "5200 Images added, 9 not loaded\n",
      "Batch Loaded 25: 473 secs\n",
      "5600 Images added, 6 not loaded\n",
      "Batch Loaded 27: 475 secs\n",
      "5800 Images added, 7 not loaded\n",
      "Batch Loaded 28: 594 secs\n",
      "5400 Images added, 5 not loaded\n",
      "Batch Loaded 26: 600 secs\n",
      "Remaining time 1901 mins\n",
      "6650 Images added, 0 not loaded\n",
      "6450 Images added, 0 not loaded\n",
      "6250 Images added, 0 not loaded\n",
      "6850 Images added, 0 not loaded\n",
      "6700 Images added, 2 not loaded\n",
      "6500 Images added, 0 not loaded\n",
      "6300 Images added, 0 not loaded\n",
      "6900 Images added, 0 not loaded\n",
      "6550 Images added, 0 not loaded\n",
      "6750 Images added, 2 not loaded\n",
      "6050 Images added, 0 not loaded\n",
      "6350 Images added, 0 not loaded\n",
      "6600 Images added, 0 not loaded\n",
      "Batch Loaded 32: 248 secs\n",
      "6950 Images added, 0 not loaded\n",
      "6800 Images added, 3 not loaded\n",
      "Batch Loaded 33: 262 secs\n",
      "6100 Images added, 1 not loaded\n",
      "6400 Images added, 0 not loaded\n",
      "Batch Loaded 31: 287 secs\n",
      "7000 Images added, 2 not loaded\n",
      "Batch Loaded 34: 307 secs\n",
      "6150 Images added, 2 not loaded\n",
      "6200 Images added, 2 not loaded\n",
      "Batch Loaded 30: 395 secs\n",
      "Remaining time 1246 mins\n",
      "7850 Images added, 0 not loaded\n",
      "7050 Images added, 0 not loaded\n",
      "7250 Images added, 1 not loaded\n",
      "7450 Images added, 0 not loaded\n",
      "7650 Images added, 0 not loaded\n",
      "7900 Images added, 0 not loaded\n",
      "7300 Images added, 1 not loaded\n",
      "7700 Images added, 1 not loaded\n",
      "7950 Images added, 0 not loaded\n",
      "7750 Images added, 1 not loaded\n",
      "8000 Images added, 0 not loaded\n",
      "Batch Loaded 39: 193 secs\n",
      "7100 Images added, 0 not loaded\n",
      "7500 Images added, 1 not loaded\n",
      "7350 Images added, 1 not loaded\n",
      "7800 Images added, 2 not loaded\n",
      "Batch Loaded 38: 294 secs\n",
      "7150 Images added, 0 not loaded\n",
      "7550 Images added, 2 not loaded\n",
      "7400 Images added, 2 not loaded\n",
      "Batch Loaded 36: 307 secs\n",
      "7200 Images added, 0 not loaded\n",
      "Batch Loaded 35: 337 secs\n",
      "7600 Images added, 3 not loaded\n",
      "Batch Loaded 37: 338 secs\n",
      "Remaining time 1057 mins\n",
      "8250 Images added, 0 not loaded\n",
      "8050 Images added, 1 not loaded\n",
      "8450 Images added, 1 not loaded\n",
      "8850 Images added, 2 not loaded\n",
      "8300 Images added, 1 not loaded\n",
      "8500 Images added, 1 not loaded\n",
      "8100 Images added, 3 not loaded\n",
      "8650 Images added, 3 not loaded\n",
      "8350 Images added, 2 not loaded\n",
      "8150 Images added, 3 not loaded\n",
      "8900 Images added, 4 not loaded\n",
      "8550 Images added, 3 not loaded\n",
      "8700 Images added, 4 not loaded\n",
      "8400 Images added, 3 not loaded\n",
      "Batch Loaded 41: 305 secs\n",
      "8200 Images added, 4 not loaded\n",
      "Batch Loaded 40: 315 secs\n",
      "8750 Images added, 4 not loaded\n",
      "8600 Images added, 7 not loaded\n",
      "Batch Loaded 42: 373 secs\n",
      "8950 Images added, 6 not loaded\n",
      "9000 Images added, 6 not loaded\n",
      "Batch Loaded 44: 432 secs\n",
      "8800 Images added, 11 not loaded\n",
      "Batch Loaded 43: 536 secs\n",
      "Remaining time 1668 mins\n",
      "9650 Images added, 0 not loaded\n",
      "9850 Images added, 0 not loaded\n",
      "9250 Images added, 0 not loaded\n",
      "9450 Images added, 2 not loaded\n",
      "9700 Images added, 0 not loaded\n",
      "9050 Images added, 1 not loaded\n",
      "9900 Images added, 0 not loaded\n",
      "9300 Images added, 2 not loaded\n",
      "9750 Images added, 0 not loaded\n",
      "9950 Images added, 0 not loaded\n",
      "9500 Images added, 4 not loaded\n",
      "9100 Images added, 4 not loaded\n",
      "9800 Images added, 0 not loaded\n",
      "Batch Loaded 48: 182 secs\n",
      "9350 Images added, 2 not loaded\n",
      "10000 Images added, 0 not loaded\n",
      "Batch Loaded 49: 195 secs\n",
      "9550 Images added, 5 not loaded\n",
      "9150 Images added, 6 not loaded\n",
      "9600 Images added, 7 not loaded\n",
      "Batch Loaded 47: 267 secs\n",
      "Batch Loaded 46: 295 secs\n",
      "9200 Images added, 8 not loaded\n",
      "Batch Loaded 45: 351 secs\n",
      "Remaining time 1090 mins\n",
      "10050 Images added, 0 not loaded\n",
      "10450 Images added, 2 not loaded\n",
      "10650 Images added, 0 not loaded\n",
      "10250 Images added, 1 not loaded\n",
      "10850 Images added, 3 not loaded\n",
      "10100 Images added, 0 not loaded\n",
      "10700 Images added, 1 not loaded\n",
      "10300 Images added, 2 not loaded\n",
      "10150 Images added, 1 not loaded\n",
      "10900 Images added, 4 not loaded\n",
      "10500 Images added, 4 not loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\PIL\\TiffImagePlugin.py:739: UserWarning: Possibly corrupt EXIF data.  Expecting to read 9830400 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\PIL\\TiffImagePlugin.py:756: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 6. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10750 Images added, 3 not loaded\n",
      "10350 Images added, 5 not loaded\n",
      "10200 Images added, 2 not loaded\n",
      "Batch Loaded 50: 282 secs\n",
      "10950 Images added, 5 not loaded\n",
      "10550 Images added, 5 not loaded\n",
      "10800 Images added, 4 not loaded\n",
      "Batch Loaded 53: 322 secs\n",
      "10400 Images added, 7 not loaded\n",
      "Batch Loaded 51: 334 secs\n",
      "11000 Images added, 7 not loaded\n",
      "Batch Loaded 54: 380 secs\n",
      "10600 Images added, 8 not loaded\n",
      "Batch Loaded 52: 415 secs\n",
      "Remaining time 1278 mins\n",
      "11450 Images added, 2 not loaded\n",
      "11050 Images added, 0 not loaded\n",
      "11650 Images added, 1 not loaded\n",
      "11850 Images added, 3 not loaded\n",
      "11500 Images added, 3 not loaded\n",
      "11250 Images added, 2 not loaded\n",
      "11100 Images added, 2 not loaded\n",
      "11700 Images added, 2 not loaded\n",
      "11150 Images added, 3 not loaded\n",
      "11300 Images added, 2 not loaded\n",
      "11200 Images added, 4 not loaded\n",
      "Batch Loaded 55: 328 secs\n",
      "11750 Images added, 4 not loaded\n",
      "11350 Images added, 5 not loaded\n",
      "11900 Images added, 5 not loaded\n",
      "11400 Images added, 5 not loaded\n",
      "Batch Loaded 56: 442 secs\n",
      "11800 Images added, 4 not loaded\n",
      "Batch Loaded 58: 446 secs\n",
      "11950 Images added, 7 not loaded\n",
      "12000 Images added, 7 not loaded\n",
      "Batch Loaded 59: 522 secs\n",
      "11550 Images added, 5 not loaded\n",
      "11600 Images added, 6 not loaded\n",
      "Batch Loaded 57: 791 secs\n",
      "Remaining time 2426 mins\n",
      "12250 Images added, 0 not loaded\n",
      "12050 Images added, 1 not loaded\n",
      "12850 Images added, 1 not loaded\n",
      "12650 Images added, 1 not loaded\n",
      "12450 Images added, 2 not loaded\n",
      "12300 Images added, 2 not loaded\n",
      "12700 Images added, 4 not loaded\n"
     ]
    }
   ],
   "source": [
    "# Create Directories for the HD5 encoded batches\n",
    "!mkdir contents\n",
    "!mkdir contents\\\\train\n",
    "!mkdir contents\\\\validation\n",
    "!mkdir contents\\\\test\n",
    "\n",
    "# Data dictionaries\n",
    "train_url      = 'C:\\\\Users\\\\User\\\\.kaggle\\\\competitions\\\\imaterialist-challenge-furniture-2018\\\\train.json'\n",
    "test_url       = 'C:\\\\Users\\\\User\\\\.kaggle\\\\competitions\\\\imaterialist-challenge-furniture-2018\\\\test.json'\n",
    "validation_url = 'C:\\\\Users\\\\User\\\\.kaggle\\\\competitions\\\\imaterialist-challenge-furniture-2018\\\\validation.json'\n",
    "\n",
    "# Load the Training Batches\n",
    "load_dispatcher(train_url, \"train\")\n",
    "\n",
    "# Load the Validation Batches\n",
    "load_dispatcher(validation_url, \"validation\")\n",
    "\n",
    "# Load the Test Batches\n",
    "load_dispatcher(test_url, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "iMaterialist-download-dataset.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
