{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0So4QvATZMA"
   },
   "source": [
    "# **iMaterialist Challenge (Furniture) at FGVC5**\n",
    "## **TFNW Kaggle Team**\n",
    "\n",
    "### **Get Started**\n",
    "\n",
    "To get started, you will:\n",
    "\n",
    "\n",
    "\n",
    "1.   Join the Completion\n",
    "2.   Create a Kaggle API key\n",
    "3.   Install the Kaggle Python module\n",
    "4.   Download the dataset for the competition\n",
    "\n",
    "**Join The Completion**\n",
    "\n",
    "Goto Kaggle.com and login.\n",
    "\n",
    "Once logged in, goto the competition page and select the JOIN COMPETITION button\n",
    "\n",
    "https://www.kaggle.com/c/imaterialist-challenge-furniture-2018\n",
    "\n",
    "**Create Kaggle API Key**\n",
    "\n",
    "To download this dataset you will use the Kaggle API for downloading datasets. To use the API, you must first create a Kaggle API Key. To create your API Key, do:\n",
    "\n",
    "\n",
    "\n",
    "1.   Click on your Profile\n",
    "2.   Select My Accont\n",
    "3.   Under API, Select Create New API Token\n",
    "4.   Download the API Key (kaggle.json)\n",
    "5.   Copy the API key to:  ~/.kaggle/kaggle.json\n",
    "6.   On Windows, that would be: \\Users\\<username>\\.kaggle\\kaggle.json\n",
    "\n",
    "**Install the Kaggle Python Module**\n",
    "\n",
    "C:> pip install kaggle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: kaggle in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n",
      "Requirement already satisfied: urllib3>=1.15 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from kaggle)\n"
     ]
    }
   ],
   "source": [
    "# Get the latest version of pip\n",
    "!python -m pip install --upgrade pip\n",
    "\n",
    "# Install the Kaggle module\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_qwEEOlZEKi"
   },
   "source": [
    "\n",
    "\n",
    "### Download the Dataset Dictionary\n",
    "\n",
    "The dataset consists of a dataset dictionary and the data. The dataset dictionary is a JSON file that contains the URL location and label for each image in the dataset. We need to download this first using the Kaggle python module:\n",
    "\n",
    "C:> kaggle competitions download -c imaterialist-challenge-furniture-2018\n",
    "\n",
    "This will place the data under:\n",
    "\n",
    "Linux/Mac: ~/.Kaggle/competitions/imaterialist-challenge-furniture-2018<br/>\n",
    "Windows: \\Users\\<username>\\.Kaggle\\competitions\\imaterialist-challenge-furniture-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json: Skipping, found more recently modified local copy (use --force to force download)\n",
      "validation.json: Skipping, found more recently modified local copy (use --force to force download)\n",
      "test.json: Skipping, found more recently modified local copy (use --force to force download)\n",
      "sample_submission_randomlabel.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c imaterialist-challenge-furniture-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will install and import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from h5py)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from h5py)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from requests)\n"
     ]
    }
   ],
   "source": [
    "# numpy for the high performance in-memory matrix/array storage and operations.\n",
    "!pip install numpy   \n",
    "# h5py for the HD5 filesystem high performance file storage of big data.\n",
    "!pip install h5py   \n",
    "# Python image manipulation library (replaces PIL)\n",
    "!pip install pillow  \n",
    "# requests for HTTP operations\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import numpy for the high performance in-memory matrix/array storage and operations.\n",
    "import numpy as np\n",
    "\n",
    "# Import h5py for the HD5 filesystem high performance file storage of big data.\n",
    "import h5py\n",
    "\n",
    "# Import PIL.Image for Python image manipulation library. \n",
    "from PIL import Image\n",
    "\n",
    "# Import json and requests for HTTP operations\n",
    "import json, requests\n",
    "\n",
    "# Import the Byte and String IO library for extracing data returned (response) frome HTTP requests.\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "# Import time to record timing\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUaKvpVUbxIm"
   },
   "source": [
    "# **Loading the Dataset (ETL)**\n",
    "\n",
    "## ** Overview **\n",
    "\n",
    "The loading and storing of the image data and corresponding data is done in several steps. To facilate minimizing the loading time and memory requirements, the process is broken into batches, and loaded and stored in concurrent (parallel) groups. \n",
    "\n",
    "Both the size of batches and the number of concurrent load/stores of batches is configurable. The overall process is as follows:\n",
    "\n",
    "1. Load the Dataset Dictionary (described below) for the training (or test or validation) data.\n",
    "2. Determine the number of images to load/store from the data dictionary.\n",
    "3. Split the loading/storing of images into batches, based on batch size.\n",
    "4. Sequentially load groups of batches, but where each batch within the batch group is concurrently processed (loaded and stored).\n",
    "\n",
    "## ** Dispatch **\n",
    "\n",
    "The function load_dispatcher() handles the dispatching of loading/storing of batches. It starts by\n",
    "taking the location of the dataset dictionary and loads it into memory. The dataset dictionary contains the location of each image and corresponding label.\n",
    "\n",
    "The dataset dictionary is in json format, as follows, where [image] is a list of image locations and [annotations] is a list of corresponding labels.\n",
    " \n",
    " {<br/>\n",
    "\"images\" : [image],<br/>\n",
    "\"annotations\" : [annotation],<br/>\n",
    "}\n",
    "\n",
    "The dispatcher makes a HTTP request (requests.get(url)) for the dataset dictionary and extracts the dataset dictionary from the contents of the response (requests.get(url).content). The dispatcher extracts it as raw byte data (ByteIO) and loads the dataset dictionary into a json format (json.load).\n",
    " \n",
    "The dispatcher calculates the number of batches based on the number of images and the batch size. The batches are then sequenced into groups (for i in range(0, batches, concurrent)), where the size of the group is the number of concurrent threads. For each group, the dispatcher creates a thread for each batch to load/store asynchronously (i.e., job). The dispatcher waits for all the concurrent jobs to complete. Based on the time to process the batch group, the dispatcher estimates the time to load/store all the remaining batches.\n",
    "\n",
    "Note, you can set the size of the batches, the size to rescale the images to, and whether to convert to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for thread execution\n",
    "import threading\n",
    "\n",
    "def load_dispatcher(url, batch_type, batch_size=200, size=(300,300), grayscale=False, concurrent=5):\n",
    "    \"\"\" Load the Data in Batches \n",
    "    url - location of data dictionary\n",
    "    batch_type - training, validation or test\n",
    "    batch_size - size of the batch\n",
    "    size - size to rescale image to\n",
    "    grayscale - flag to convert image to grayscale\n",
    "    concurrent - the number of concurrent (parallel)) batch loads\n",
    "    \"\"\"\n",
    "    \n",
    "    # First retreive the dataset dictionary, which is in a JSON format. \n",
    "    # Dictionary is stored remote: We will make a HTTP request\n",
    "    if url.startswith(\"http\"):\n",
    "        datadict = json.load( requests.get(url).content )\n",
    "    # Dictionary is stored locally\n",
    "    else:\n",
    "        datadict = json.load( open( url ) )\n",
    "   \n",
    "    # The number of batches\n",
    "    batches = int(len(datadict['images']) / batch_size)\n",
    "    \n",
    "    # Sequentially Load each batch group (i.e., concurrent)\n",
    "    for i in range(0, batches, concurrent):\n",
    "        # Start time for the batch group\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Listof threads, corresponding to to the processing of each batch in the batch group\n",
    "        threads = []\n",
    "        # Create and Start a processing thread for each batch in the batch group\n",
    "        for j in range(concurrent):\n",
    "            t = threading.Thread(target=load_and_store_batch, args=(datadict, batch_type, i + j, batch_size, size, grayscale,))\n",
    "            # Keep track (remember) of the thread\n",
    "            threads.append(t)\n",
    "            # Start the thread\n",
    "            t.start()\n",
    "        # Join the threads into a single wait for all threads to complete\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "                  \n",
    "        # Calculate elapsed time in seconds to load this batch group\n",
    "        elapse = int(time.time() - start_time)\n",
    "            \n",
    "        # Estimate remaining time in minutes for loading remaining barches.\n",
    "        remaining = int( ( ( batches - i ) / concurrent ) * elapse ) / 60\n",
    "        \n",
    "        print(\"Remaining time %d mins\" % remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Load and Store the Data Batches **\n",
    "\n",
    "We have a mass amount of images: 194,828 training, 6,400 validation, 12,800 test. If we tried to load it all, we would need 54GB of memory! \n",
    "\n",
    "Instead, we will load the data into smaller training (validation and test) batches and store them separately on disk. We use the loaded data dictionary and sequentially move through it (batch_size at a time) to build the batches and save them into a HD5 high performance file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_store_batch(datadict, batch_type, pos, batch_size, size, grayscale):\n",
    "    \"\"\" Process loading (extration), handling (transformation) and storing (loading) as a batch \n",
    "    batch_type - training, validation or test\n",
    "    pos - the batch slice position in the data (i.e., the first, the second, etc)\n",
    "    batch_size - size of the batch\n",
    "    size - size to rescale image to\n",
    "    grayscale - flag to convert image to grayscale\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    start = pos * batch_size\n",
    "    images, labels = load_batch(datadict, start, batch_size, size, grayscale )\n",
    "        \n",
    "    # Calculate elapsed time in seconds to load this batch\n",
    "    elapse = int(time.time() - start_time)\n",
    "        \n",
    "    print(\"Batch Loaded %d: %d secs\" % (pos, elapse))\n",
    "        \n",
    "    # Write the batch to disk as HD5 file\n",
    "    with h5py.File('contents\\\\' + batch_type + '\\\\images' + str(pos) + '.h5', 'w') as hf:\n",
    "        hf.create_dataset(\"images\",  data=images)\n",
    "    with h5py.File('contents\\\\' + batch_type + '\\\\labels' + str(pos) +  '.h5', 'w') as hf:\n",
    "        hf.create_dataset(\"labels\",  data=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Extraction / Dataset Directory **\n",
    "\n",
    "The load_batch() function uses the location of each image specified in the slice of the dataset dictionary to load the images into memory, do transformations and store the images on disk, as a batch. The slice is defined by a start position in the dataset dictionary and length, denoted by batch_size.\n",
    "\n",
    "## **Transform / Load**\n",
    " \n",
    "The images are all RGB images, but are of different pixel sizes. For the neural network, they all need to be the same size. We will rescale each of our images to be 300 by 300 pixels (default), but you can choose another scale with the size parameter. THe image data will then be packed into a high performance numpy 3D matrix. The row/column are the height and width (300,300) and the third dimension are the channels (3).\n",
    "\n",
    "The images will be stored in the list images[] and the corresponding labels in the list labels[].\n",
    "\n",
    "The load_batch() function loads a batch of images from the training, validation or test data and does the transform function. The images and corresponding labels are then returned as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n_j3e0kiZELK"
   },
   "outputs": [],
   "source": [
    "def load_batch(datadict, start, batch_size=200, size=(300,300), grayscale=False ):\n",
    "    \"\"\" Load the training dataset \n",
    "    datadict - data image/label dictionary\n",
    "    start - index to start reading batch of images\n",
    "    batch_size - number of images to read (None = all images)\n",
    "    grayscale - flag if image should be converted to grayscale\n",
    "    \"\"\"\n",
    "    \n",
    "    images = [] # List containing the images\n",
    "    labels = [] # List containing the corresponding labels for the images\n",
    "    \n",
    "    # Number of images to load\n",
    "    if batch_size == None:\n",
    "        batch_size = len(datadict['images'])\n",
    "      \n",
    "    # Final shape of image Height, Width\n",
    "    if grayscale == True:\n",
    "        shape = size\n",
    "    # Final shape of image Height, Width, Channels(3)\n",
    "    else:\n",
    "        shape = size + (3,)\n",
    "            \n",
    "    # Load the batch of images/labels from the Data Dictionary\n",
    "    end = start + batch_size\n",
    "    for i in range(start, end): \n",
    "        image_url = datadict['images'][i]['url'][0]\n",
    "        label_id  = datadict['annotations'][i]['label_id']\n",
    "        \n",
    "        not_loaded = 0\n",
    "\n",
    "        # Download, resize and convert images to arrays\n",
    "        try:\n",
    "            # Make HTTP request fot the image data\n",
    "            response = requests.get(image_url, timeout=5)\n",
    "            \n",
    "            # Use the PIL.Image libary to load the image data as au uncompressed RGB or Grayscale bitmap\n",
    "            if grayscale == True:\n",
    "                image = Image.open(BytesIO(response.content)).convert('LA')\n",
    "            else:\n",
    "                image = Image.open(BytesIO(response.content))\n",
    "            \n",
    "            # Resize the image to be all the same size\n",
    "            image = image.resize(size, resample=Image.LANCZOS)\n",
    "            \n",
    "            # Load the image into a 3D numpy array\n",
    "            image = np.asarray(image)\n",
    "            \n",
    "            # Discard image if it does not fit the final shape\n",
    "            assert image.shape == shape\n",
    "        except Exception as ex:\n",
    "            not_loaded += 1\n",
    "            continue\n",
    "        \n",
    "        # if bad image, skip\n",
    "        if np.any(image == None):\n",
    "            continue\n",
    "        # add image to images list\n",
    "        images.append( image )\n",
    "        # add corresponding label to labels list\n",
    "        labels.append( label_id )\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            print('%d Images added, %d not loaded' % ((i + 1), not_loaded))\n",
    "\n",
    "    return images, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Load\n",
    "\n",
    "The URLs below are for more laptop. You will need to modify it to the location on your laptop.\n",
    "\n",
    "Running as 5 concurrent processes in batches of 200, takes 1/2 day on my laptop with my local Internet service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KqrObUaMZELc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 Images added, 0 not loaded\n",
      "50 Images added, 0 not loaded\n",
      "650 Images added, 0 not loaded\n",
      "450 Images added, 0 not loaded\n",
      "850 Images added, 0 not loaded\n",
      "100 Images added, 0 not loaded\n",
      "300 Images added, 0 not loaded\n",
      "700 Images added, 0 not loaded\n",
      "500 Images added, 0 not loaded\n",
      "900 Images added, 0 not loaded\n",
      "150 Images added, 0 not loaded\n",
      "750 Images added, 0 not loaded\n",
      "950 Images added, 0 not loaded\n",
      "350 Images added, 0 not loaded\n",
      "550 Images added, 0 not loaded\n",
      "200 Images added, 0 not loaded\n",
      "Batch Loaded 0: 198 secs\n",
      "1000 Images added, 0 not loaded\n",
      "Batch Loaded 4: 243 secs\n",
      "400 Images added, 0 not loaded\n",
      "Batch Loaded 1: 244 secs\n",
      "600 Images added, 0 not loaded\n",
      "Batch Loaded 2: 249 secs\n",
      "800 Images added, 0 not loaded\n",
      "Batch Loaded 3: 267 secs\n",
      "Remaining time 866 mins\n",
      "1450 Images added, 0 not loaded\n",
      "1050 Images added, 0 not loaded\n",
      "1250 Images added, 0 not loaded\n",
      "1650 Images added, 0 not loaded\n",
      "1850 Images added, 0 not loaded\n",
      "1500 Images added, 0 not loaded\n",
      "1300 Images added, 0 not loaded\n",
      "1900 Images added, 0 not loaded\n",
      "1100 Images added, 0 not loaded\n",
      "1700 Images added, 0 not loaded\n",
      "1550 Images added, 0 not loaded\n",
      "1950 Images added, 0 not loaded\n",
      "1150 Images added, 0 not loaded\n",
      "1350 Images added, 0 not loaded\n",
      "1750 Images added, 0 not loaded\n",
      "1600 Images added, 0 not loaded\n",
      "Batch Loaded 7: 201 secs\n",
      "2000 Images added, 0 not loaded\n",
      "Batch Loaded 9: 224 secs\n",
      "1800 Images added, 0 not loaded\n",
      "Batch Loaded 8: 233 secs\n",
      "1200 Images added, 0 not loaded\n",
      "Batch Loaded 5: 266 secs\n",
      "1400 Images added, 0 not loaded\n",
      "Batch Loaded 6: 269 secs\n",
      "Remaining time 868 mins\n",
      "2850 Images added, 0 not loaded\n",
      "2450 Images added, 0 not loaded\n",
      "2650 Images added, 0 not loaded\n",
      "2050 Images added, 0 not loaded\n",
      "2900 Images added, 0 not loaded\n",
      "2500 Images added, 0 not loaded\n",
      "2250 Images added, 0 not loaded\n",
      "2100 Images added, 0 not loaded\n",
      "2950 Images added, 0 not loaded\n",
      "2550 Images added, 0 not loaded\n",
      "2750 Images added, 0 not loaded\n",
      "2300 Images added, 0 not loaded\n",
      "3000 Images added, 0 not loaded\n",
      "Batch Loaded 14: 272 secs\n",
      "2600 Images added, 0 not loaded\n",
      "Batch Loaded 12: 295 secs\n",
      "2800 Images added, 0 not loaded\n",
      "Batch Loaded 13: 319 secs\n",
      "2350 Images added, 0 not loaded\n",
      "2200 Images added, 0 not loaded\n",
      "Batch Loaded 10: 360 secs\n",
      "2400 Images added, 0 not loaded\n",
      "Batch Loaded 11: 383 secs\n",
      "Remaining time 1230 mins\n",
      "3650 Images added, 0 not loaded\n",
      "3050 Images added, 0 not loaded\n",
      "3250 Images added, 0 not loaded\n",
      "3850 Images added, 0 not loaded\n",
      "3450 Images added, 0 not loaded\n",
      "3100 Images added, 0 not loaded\n",
      "3900 Images added, 0 not loaded\n",
      "3700 Images added, 0 not loaded\n",
      "3500 Images added, 0 not loaded\n",
      "3950 Images added, 0 not loaded\n",
      "3550 Images added, 0 not loaded\n",
      "3750 Images added, 0 not loaded\n",
      "3300 Images added, 0 not loaded\n",
      "4000 Images added, 0 not loaded\n",
      "Batch Loaded 19: 267 secs\n",
      "3600 Images added, 0 not loaded\n",
      "Batch Loaded 17: 270 secs\n",
      "3200 Images added, 0 not loaded\n",
      "Batch Loaded 15: 279 secs\n",
      "3350 Images added, 0 not loaded\n",
      "3800 Images added, 0 not loaded\n",
      "Batch Loaded 18: 294 secs\n",
      "3400 Images added, 0 not loaded\n",
      "Batch Loaded 16: 321 secs\n",
      "Remaining time 1029 mins\n",
      "4650 Images added, 0 not loaded\n",
      "4050 Images added, 0 not loaded\n",
      "4450 Images added, 0 not loaded\n",
      "4250 Images added, 0 not loaded\n",
      "4850 Images added, 0 not loaded\n",
      "4100 Images added, 0 not loaded\n",
      "4700 Images added, 0 not loaded\n",
      "4500 Images added, 0 not loaded\n",
      "4300 Images added, 0 not loaded\n",
      "4900 Images added, 0 not loaded\n",
      "4150 Images added, 0 not loaded\n",
      "4550 Images added, 0 not loaded\n",
      "4350 Images added, 0 not loaded\n",
      "4750 Images added, 0 not loaded\n",
      "4950 Images added, 0 not loaded\n",
      "4400 Images added, 0 not loaded\n",
      "Batch Loaded 21: 258 secs\n",
      "5000 Images added, 0 not loaded\n",
      "Batch Loaded 24: 278 secs\n",
      "4600 Images added, 0 not loaded\n",
      "Batch Loaded 22: 293 secs\n",
      "4800 Images added, 0 not loaded\n",
      "Batch Loaded 23: 306 secs\n",
      "4200 Images added, 0 not loaded\n",
      "Batch Loaded 20: 327 secs\n",
      "Remaining time 1039 mins\n",
      "5050 Images added, 0 not loaded\n",
      "5250 Images added, 0 not loaded\n",
      "5450 Images added, 0 not loaded\n",
      "5100 Images added, 0 not loaded\n",
      "5900 Images added, 0 not loaded\n",
      "5700 Images added, 0 not loaded\n",
      "5500 Images added, 0 not loaded\n",
      "5300 Images added, 0 not loaded\n",
      "5950 Images added, 0 not loaded\n",
      "5150 Images added, 0 not loaded\n",
      "5350 Images added, 0 not loaded\n",
      "5550 Images added, 0 not loaded\n",
      "5750 Images added, 0 not loaded\n",
      "6000 Images added, 0 not loaded\n",
      "Batch Loaded 29: 331 secs\n",
      "5200 Images added, 0 not loaded\n",
      "Batch Loaded 25: 348 secs\n",
      "5600 Images added, 0 not loaded\n",
      "Batch Loaded 27: 359 secs\n",
      "5800 Images added, 0 not loaded\n",
      "Batch Loaded 28: 360 secs\n",
      "5400 Images added, 0 not loaded\n",
      "Batch Loaded 26: 369 secs\n",
      "Remaining time 1167 mins\n",
      "6450 Images added, 0 not loaded\n",
      "6650 Images added, 0 not loaded\n",
      "6850 Images added, 0 not loaded\n",
      "6500 Images added, 0 not loaded\n",
      "6050 Images added, 0 not loaded\n",
      "6250 Images added, 0 not loaded\n",
      "6700 Images added, 0 not loaded\n",
      "6900 Images added, 0 not loaded\n",
      "6100 Images added, 0 not loaded\n",
      "6950 Images added, 0 not loaded\n",
      "6300 Images added, 0 not loaded\n",
      "6550 Images added, 0 not loaded\n",
      "6750 Images added, 0 not loaded\n",
      "6150 Images added, 0 not loaded\n",
      "6350 Images added, 0 not loaded\n",
      "6600 Images added, 0 not loaded\n",
      "Batch Loaded 32: 279 secs\n",
      "7000 Images added, 0 not loaded\n",
      "Batch Loaded 34: 303 secs\n",
      "6800 Images added, 0 not loaded\n",
      "Batch Loaded 33: 317 secs\n",
      "6200 Images added, 0 not loaded\n",
      "Batch Loaded 30: 339 secs\n",
      "6400 Images added, 0 not loaded\n",
      "Batch Loaded 31: 342 secs\n",
      "Remaining time 1076 mins\n",
      "7050 Images added, 0 not loaded\n",
      "7250 Images added, 0 not loaded\n",
      "7650 Images added, 0 not loaded\n",
      "7850 Images added, 0 not loaded\n",
      "7450 Images added, 0 not loaded\n",
      "7900 Images added, 0 not loaded\n",
      "7300 Images added, 0 not loaded\n",
      "7100 Images added, 0 not loaded\n",
      "7700 Images added, 0 not loaded\n",
      "7950 Images added, 0 not loaded\n",
      "7550 Images added, 0 not loaded\n",
      "7150 Images added, 0 not loaded\n",
      "7350 Images added, 0 not loaded\n",
      "8000 Images added, 0 not loaded\n",
      "Batch Loaded 39: 236 secs\n",
      "7600 Images added, 0 not loaded\n",
      "Batch Loaded 37: 250 secs\n",
      "7200 Images added, 0 not loaded\n",
      "Batch Loaded 35: 255 secs\n",
      "7750 Images added, 0 not loaded\n",
      "7400 Images added, 0 not loaded\n",
      "Batch Loaded 36: 295 secs\n",
      "7800 Images added, 0 not loaded\n",
      "Batch Loaded 38: 322 secs\n",
      "Remaining time 1010 mins\n",
      "8450 Images added, 0 not loaded\n",
      "8050 Images added, 0 not loaded\n",
      "8250 Images added, 0 not loaded\n",
      "8850 Images added, 0 not loaded\n",
      "8100 Images added, 0 not loaded\n",
      "8500 Images added, 0 not loaded\n",
      "8300 Images added, 0 not loaded\n",
      "8350 Images added, 0 not loaded\n",
      "8650 Images added, 0 not loaded\n",
      "8550 Images added, 0 not loaded\n",
      "8900 Images added, 0 not loaded\n",
      "8200 Images added, 0 not loaded\n",
      "Batch Loaded 40: 234 secs\n",
      "8400 Images added, 0 not loaded\n",
      "Batch Loaded 41: 248 secs\n",
      "8700 Images added, 0 not loaded\n",
      "8950 Images added, 0 not loaded\n",
      "8600 Images added, 0 not loaded\n",
      "Batch Loaded 42: 298 secs\n",
      "8750 Images added, 0 not loaded\n",
      "9000 Images added, 0 not loaded\n",
      "Batch Loaded 44: 355 secs\n",
      "8800 Images added, 0 not loaded\n",
      "Batch Loaded 43: 385 secs\n",
      "Remaining time 1198 mins\n",
      "9650 Images added, 0 not loaded\n",
      "9850 Images added, 0 not loaded\n",
      "9450 Images added, 0 not loaded\n",
      "9050 Images added, 0 not loaded\n",
      "9700 Images added, 0 not loaded\n",
      "9900 Images added, 0 not loaded\n",
      "9250 Images added, 0 not loaded\n",
      "9950 Images added, 0 not loaded\n",
      "9750 Images added, 0 not loaded\n",
      "9100 Images added, 0 not loaded\n",
      "9500 Images added, 0 not loaded\n",
      "10000 Images added, 0 not loaded\n",
      "Batch Loaded 49: 180 secs\n",
      "9300 Images added, 0 not loaded\n",
      "9800 Images added, 0 not loaded\n",
      "Batch Loaded 48: 208 secs\n",
      "9150 Images added, 0 not loaded\n",
      "9550 Images added, 0 not loaded\n",
      "9350 Images added, 0 not loaded\n",
      "9200 Images added, 0 not loaded\n",
      "Batch Loaded 45: 314 secs\n",
      "9600 Images added, 0 not loaded\n",
      "Batch Loaded 47: 322 secs\n",
      "9400 Images added, 0 not loaded\n",
      "Batch Loaded 46: 373 secs\n",
      "Remaining time 1155 mins\n",
      "10650 Images added, 0 not loaded\n",
      "10250 Images added, 0 not loaded\n",
      "10850 Images added, 0 not loaded\n",
      "10450 Images added, 0 not loaded\n",
      "10050 Images added, 0 not loaded\n",
      "10300 Images added, 0 not loaded\n",
      "10700 Images added, 0 not loaded\n",
      "10100 Images added, 0 not loaded\n",
      "10500 Images added, 0 not loaded\n",
      "10900 Images added, 0 not loaded\n",
      "10350 Images added, 0 not loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\PIL\\TiffImagePlugin.py:739: UserWarning: Possibly corrupt EXIF data.  Expecting to read 9830400 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\PIL\\TiffImagePlugin.py:756: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 6. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10150 Images added, 0 not loaded\n",
      "10950 Images added, 0 not loaded\n",
      "10750 Images added, 0 not loaded\n",
      "10550 Images added, 0 not loaded\n",
      "10200 Images added, 0 not loaded\n",
      "Batch Loaded 50: 251 secs\n",
      "11000 Images added, 0 not loaded\n",
      "Batch Loaded 54: 260 secs\n",
      "10800 Images added, 0 not loaded\n",
      "Batch Loaded 53: 266 secs\n",
      "10600 Images added, 0 not loaded\n",
      "Batch Loaded 52: 276 secs\n",
      "10400 Images added, 0 not loaded\n",
      "Batch Loaded 51: 277 secs\n",
      "Remaining time 853 mins\n",
      "11450 Images added, 0 not loaded\n",
      "11650 Images added, 0 not loaded\n",
      "11850 Images added, 0 not loaded\n",
      "11050 Images added, 0 not loaded\n",
      "11500 Images added, 0 not loaded\n",
      "11900 Images added, 0 not loaded\n",
      "11700 Images added, 0 not loaded\n",
      "11300 Images added, 0 not loaded\n",
      "11100 Images added, 0 not loaded\n",
      "11750 Images added, 0 not loaded\n",
      "11550 Images added, 0 not loaded\n",
      "11350 Images added, 0 not loaded\n",
      "11150 Images added, 0 not loaded\n",
      "Batch Loaded 59: 3304 secs\n",
      "11800 Images added, 0 not loaded\n",
      "Batch Loaded 58: 3324 secs\n",
      "11600 Images added, 0 not loaded\n",
      "Batch Loaded 57: 3334 secs\n",
      "11400 Images added, 0 not loaded\n",
      "Batch Loaded 56: 3340 secs\n",
      "11200 Images added, 0 not loaded\n",
      "Batch Loaded 55: 3353 secs\n",
      "Remaining time 10274 mins\n",
      "12850 Images added, 0 not loaded\n",
      "12450 Images added, 0 not loaded\n",
      "12050 Images added, 0 not loaded\n",
      "12650 Images added, 0 not loaded\n",
      "12300 Images added, 0 not loaded\n",
      "12500 Images added, 0 not loaded\n",
      "12700 Images added, 0 not loaded\n",
      "12100 Images added, 0 not loaded\n",
      "12350 Images added, 0 not loaded\n",
      "12950 Images added, 0 not loaded\n",
      "12550 Images added, 0 not loaded\n",
      "12750 Images added, 0 not loaded\n",
      "Batch Loaded 61: 284 secs\n",
      "12150 Images added, 0 not loaded\n",
      "13000 Images added, 0 not loaded\n",
      "Batch Loaded 64: 317 secs\n",
      "12800 Images added, 0 not loaded\n",
      "Batch Loaded 63: 332 secs\n",
      "12600 Images added, 0 not loaded\n",
      "Batch Loaded 62: 332 secs\n",
      "12200 Images added, 0 not loaded\n",
      "Batch Loaded 60: 344 secs\n",
      "Remaining time 1051 mins\n",
      "13050 Images added, 0 not loaded\n",
      "13650 Images added, 0 not loaded\n",
      "13250 Images added, 0 not loaded\n",
      "13450 Images added, 0 not loaded\n",
      "13850 Images added, 0 not loaded\n",
      "13300 Images added, 0 not loaded\n",
      "13100 Images added, 0 not loaded\n",
      "13700 Images added, 0 not loaded\n",
      "13350 Images added, 0 not loaded\n",
      "13900 Images added, 0 not loaded\n",
      "13500 Images added, 0 not loaded\n",
      "13150 Images added, 0 not loaded\n",
      "13750 Images added, 0 not loaded\n",
      "13200 Images added, 0 not loaded\n",
      "Batch Loaded 65: 245 secs\n",
      "13400 Images added, 0 not loaded\n",
      "Batch Loaded 66: 246 secs\n",
      "13950 Images added, 0 not loaded\n",
      "13550 Images added, 0 not loaded\n",
      "13800 Images added, 0 not loaded\n",
      "Batch Loaded 68: 297 secs\n",
      "14000 Images added, 0 not loaded\n",
      "Batch Loaded 69: 332 secs\n",
      "13600 Images added, 0 not loaded\n",
      "Batch Loaded 67: 337 secs\n",
      "Remaining time 1024 mins\n",
      "14450 Images added, 0 not loaded\n",
      "14650 Images added, 0 not loaded\n",
      "14850 Images added, 0 not loaded\n",
      "14050 Images added, 0 not loaded\n",
      "14250 Images added, 0 not loaded\n",
      "14700 Images added, 0 not loaded\n",
      "14100 Images added, 0 not loaded\n",
      "14500 Images added, 0 not loaded\n",
      "14300 Images added, 0 not loaded\n",
      "14900 Images added, 0 not loaded\n",
      "14150 Images added, 0 not loaded\n",
      "14750 Images added, 0 not loaded\n",
      "14550 Images added, 0 not loaded\n",
      "14950 Images added, 0 not loaded\n",
      "14350 Images added, 0 not loaded\n",
      "14200 Images added, 0 not loaded\n",
      "Batch Loaded 70: 260 secs\n",
      "14800 Images added, 0 not loaded\n",
      "Batch Loaded 73: 265 secs\n",
      "14600 Images added, 0 not loaded\n",
      "Batch Loaded 72: 291 secs\n",
      "15000 Images added, 0 not loaded\n",
      "Batch Loaded 74: 305 secs\n",
      "14400 Images added, 0 not loaded\n",
      "Batch Loaded 71: 310 secs\n",
      "Remaining time 934 mins\n",
      "15650 Images added, 0 not loaded\n",
      "15850 Images added, 0 not loaded\n",
      "15050 Images added, 0 not loaded\n",
      "15450 Images added, 0 not loaded\n",
      "15900 Images added, 0 not loaded\n",
      "15700 Images added, 0 not loaded\n",
      "15250 Images added, 0 not loaded\n",
      "15500 Images added, 0 not loaded\n",
      "15100 Images added, 0 not loaded\n",
      "15750 Images added, 0 not loaded\n",
      "15550 Images added, 0 not loaded\n",
      "15150 Images added, 0 not loaded\n",
      "15300 Images added, 0 not loaded\n",
      "15950 Images added, 0 not loaded\n",
      "15800 Images added, 0 not loaded\n",
      "Batch Loaded 78: 239 secs\n",
      "15600 Images added, 0 not loaded\n",
      "Batch Loaded 77: 273 secs\n",
      "16000 Images added, 0 not loaded\n",
      "Batch Loaded 79: 288 secs\n",
      "15200 Images added, 0 not loaded\n",
      "Batch Loaded 75: 300 secs\n",
      "15350 Images added, 0 not loaded\n",
      "15400 Images added, 0 not loaded\n",
      "Batch Loaded 76: 429 secs\n",
      "Remaining time 1288 mins\n",
      "16850 Images added, 0 not loaded\n",
      "16250 Images added, 0 not loaded\n",
      "16050 Images added, 0 not loaded\n",
      "16650 Images added, 0 not loaded\n",
      "16450 Images added, 0 not loaded\n",
      "16900 Images added, 0 not loaded\n",
      "16100 Images added, 0 not loaded\n",
      "16300 Images added, 0 not loaded\n",
      "16500 Images added, 0 not loaded\n",
      "16700 Images added, 0 not loaded\n",
      "16950 Images added, 0 not loaded\n",
      "16350 Images added, 0 not loaded\n",
      "16750 Images added, 0 not loaded\n",
      "16150 Images added, 0 not loaded\n",
      "16400 Images added, 0 not loaded\n",
      "Batch Loaded 81: 212 secs\n",
      "Batch Loaded 84: 217 secs\n",
      "16800 Images added, 0 not loaded\n",
      "Batch Loaded 83: 220 secs\n",
      "16600 Images added, 0 not loaded\n",
      "Batch Loaded 82: 220 secs\n",
      "16200 Images added, 0 not loaded\n",
      "Batch Loaded 80: 251 secs\n",
      "Remaining time 750 mins\n",
      "17250 Images added, 0 not loaded\n",
      "17650 Images added, 0 not loaded\n",
      "17050 Images added, 0 not loaded\n",
      "17450 Images added, 0 not loaded\n",
      "17300 Images added, 0 not loaded\n",
      "17100 Images added, 0 not loaded\n",
      "17850 Images added, 0 not loaded\n",
      "17500 Images added, 0 not loaded\n",
      "17700 Images added, 0 not loaded\n",
      "17150 Images added, 0 not loaded\n",
      "17350 Images added, 0 not loaded\n",
      "17550 Images added, 0 not loaded\n",
      "17900 Images added, 0 not loaded\n",
      "17750 Images added, 0 not loaded\n",
      "17200 Images added, 0 not loaded\n",
      "Batch Loaded 85: 227 secs\n",
      "17400 Images added, 0 not loaded\n",
      "Batch Loaded 86: 234 secs\n",
      "17950 Images added, 0 not loaded\n",
      "17600 Images added, 0 not loaded\n",
      "Batch Loaded 87: 253 secs\n",
      "18000 Images added, 0 not loaded\n",
      "Batch Loaded 89: 314 secs\n",
      "Batch Loaded 88: 395 secs\n",
      "Remaining time 1173 mins\n",
      "18250 Images added, 0 not loaded\n",
      "18050 Images added, 0 not loaded\n",
      "18650 Images added, 0 not loaded\n",
      "18450 Images added, 0 not loaded\n",
      "18850 Images added, 0 not loaded\n",
      "18300 Images added, 0 not loaded\n",
      "18500 Images added, 0 not loaded\n",
      "18100 Images added, 0 not loaded\n",
      "18900 Images added, 0 not loaded\n",
      "18700 Images added, 0 not loaded\n",
      "18550 Images added, 0 not loaded\n",
      "18750 Images added, 0 not loaded\n",
      "18950 Images added, 0 not loaded\n",
      "18150 Images added, 0 not loaded\n",
      "18800 Images added, 0 not loaded\n",
      "Batch Loaded 93: 261 secs\n",
      "18400 Images added, 0 not loaded\n",
      "Batch Loaded 91: 264 secs\n",
      "19000 Images added, 0 not loaded\n",
      "Batch Loaded 94: 267 secs\n",
      "18200 Images added, 0 not loaded\n",
      "Batch Loaded 90: 271 secs\n",
      "18600 Images added, 0 not loaded\n",
      "Batch Loaded 92: 337 secs\n",
      "Remaining time 998 mins\n",
      "19250 Images added, 0 not loaded\n",
      "19650 Images added, 0 not loaded\n",
      "19050 Images added, 0 not loaded\n",
      "19450 Images added, 0 not loaded\n",
      "19850 Images added, 0 not loaded\n",
      "19300 Images added, 0 not loaded\n",
      "19500 Images added, 0 not loaded\n",
      "19100 Images added, 0 not loaded\n",
      "19900 Images added, 0 not loaded\n",
      "19550 Images added, 0 not loaded\n",
      "19150 Images added, 0 not loaded\n",
      "19350 Images added, 0 not loaded\n",
      "19600 Images added, 0 not loaded\n",
      "Batch Loaded 97: 252 secs\n",
      "19200 Images added, 0 not loaded\n",
      "Batch Loaded 95: 268 secs\n",
      "19400 Images added, 0 not loaded\n",
      "Batch Loaded 96: 284 secs\n",
      "19700 Images added, 0 not loaded\n",
      "19950 Images added, 0 not loaded\n",
      "19750 Images added, 0 not loaded\n",
      "19800 Images added, 0 not loaded\n",
      "Batch Loaded 98: 416 secs\n",
      "20000 Images added, 0 not loaded\n",
      "Batch Loaded 99: 461 secs\n",
      "Remaining time 1353 mins\n",
      "20450 Images added, 0 not loaded\n",
      "20850 Images added, 0 not loaded\n",
      "20650 Images added, 0 not loaded\n",
      "20250 Images added, 0 not loaded\n",
      "20050 Images added, 0 not loaded\n",
      "20900 Images added, 0 not loaded\n",
      "20100 Images added, 0 not loaded\n",
      "20500 Images added, 0 not loaded\n",
      "20300 Images added, 0 not loaded\n",
      "20950 Images added, 0 not loaded\n",
      "20150 Images added, 0 not loaded\n",
      "20550 Images added, 0 not loaded\n",
      "20350 Images added, 0 not loaded\n",
      "20750 Images added, 0 not loaded\n",
      "21000 Images added, 0 not loaded\n",
      "Batch Loaded 104: 278 secs\n",
      "20800 Images added, 0 not loaded\n",
      "Batch Loaded 103: 315 secs\n",
      "20200 Images added, 0 not loaded\n",
      "Batch Loaded 100: 325 secs\n",
      "20600 Images added, 0 not loaded\n",
      "Batch Loaded 102: 335 secs\n",
      "20400 Images added, 0 not loaded\n",
      "Batch Loaded 101: 405 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining time 1182 mins\n",
      "21450 Images added, 0 not loaded\n",
      "21250 Images added, 0 not loaded\n",
      "21650 Images added, 0 not loaded\n",
      "21850 Images added, 0 not loaded\n",
      "21500 Images added, 0 not loaded\n",
      "21700 Images added, 0 not loaded\n",
      "21900 Images added, 0 not loaded\n",
      "21050 Images added, 0 not loaded\n",
      "21550 Images added, 0 not loaded\n",
      "21300 Images added, 0 not loaded\n",
      "21750 Images added, 0 not loaded\n",
      "21950 Images added, 0 not loaded\n",
      "21100 Images added, 0 not loaded\n",
      "21350 Images added, 0 not loaded\n",
      "22000 Images added, 0 not loaded\n",
      "Batch Loaded 109: 234 secs\n",
      "21600 Images added, 0 not loaded\n",
      "Batch Loaded 107: 236 secs\n",
      "21800 Images added, 0 not loaded\n",
      "Batch Loaded 108: 281 secs\n",
      "21400 Images added, 0 not loaded\n",
      "Batch Loaded 106: 298 secs\n",
      "21150 Images added, 0 not loaded\n",
      "21200 Images added, 0 not loaded\n",
      "Batch Loaded 105: 398 secs\n",
      "Remaining time 1152 mins\n",
      "22250 Images added, 0 not loaded\n",
      "22650 Images added, 0 not loaded\n",
      "22450 Images added, 0 not loaded\n",
      "22050 Images added, 0 not loaded\n",
      "22850 Images added, 0 not loaded\n",
      "22700 Images added, 0 not loaded\n",
      "22900 Images added, 0 not loaded\n",
      "22300 Images added, 0 not loaded\n",
      "22100 Images added, 0 not loaded\n",
      "22500 Images added, 0 not loaded\n",
      "22750 Images added, 0 not loaded\n",
      "22950 Images added, 0 not loaded\n",
      "22150 Images added, 0 not loaded\n",
      "22350 Images added, 0 not loaded\n",
      "22550 Images added, 0 not loaded\n",
      "22800 Images added, 0 not loaded\n",
      "Batch Loaded 113: 231 secs\n",
      "23000 Images added, 0 not loaded\n",
      "Batch Loaded 114: 236 secs\n",
      "22400 Images added, 0 not loaded\n",
      "Batch Loaded 111: 265 secs\n",
      "22600 Images added, 0 not loaded\n",
      "Batch Loaded 112: 268 secs\n",
      "22200 Images added, 0 not loaded\n",
      "Batch Loaded 110: 270 secs\n",
      "Remaining time 777 mins\n",
      "23450 Images added, 0 not loaded\n",
      "23250 Images added, 0 not loaded\n",
      "23650 Images added, 0 not loaded\n",
      "23850 Images added, 0 not loaded\n",
      "23500 Images added, 0 not loaded\n",
      "23300 Images added, 0 not loaded\n",
      "23900 Images added, 0 not loaded\n",
      "23700 Images added, 0 not loaded\n",
      "23050 Images added, 0 not loaded\n",
      "23550 Images added, 0 not loaded\n",
      "23350 Images added, 0 not loaded\n",
      "23950 Images added, 0 not loaded\n",
      "23100 Images added, 0 not loaded\n",
      "23750 Images added, 0 not loaded\n",
      "24000 Images added, 0 not loaded\n",
      "Batch Loaded 119: 253 secs\n",
      "23400 Images added, 0 not loaded\n",
      "Batch Loaded 116: 254 secs\n",
      "23600 Images added, 0 not loaded\n",
      "Batch Loaded 117: 273 secs\n",
      "23150 Images added, 0 not loaded\n",
      "23800 Images added, 0 not loaded\n",
      "Batch Loaded 118: 317 secs\n",
      "23200 Images added, 0 not loaded\n",
      "Batch Loaded 115: 328 secs\n",
      "Remaining time 939 mins\n",
      "24250 Images added, 0 not loaded\n",
      "24850 Images added, 0 not loaded\n",
      "24450 Images added, 0 not loaded\n",
      "24300 Images added, 0 not loaded\n",
      "24050 Images added, 0 not loaded\n",
      "24700 Images added, 0 not loaded\n",
      "24500 Images added, 0 not loaded\n",
      "24900 Images added, 0 not loaded\n",
      "24350 Images added, 0 not loaded\n",
      "24750 Images added, 0 not loaded\n",
      "24100 Images added, 0 not loaded\n",
      "24550 Images added, 0 not loaded\n",
      "24400 Images added, 0 not loaded\n",
      "Batch Loaded 121: 280 secs\n",
      "24950 Images added, 0 not loaded\n",
      "24600 Images added, 0 not loaded\n",
      "Batch Loaded 122: 359 secs\n",
      "25000 Images added, 0 not loaded\n",
      "Batch Loaded 124: 369 secs\n",
      "Batch Loaded 123: 388 secs\n",
      "24150 Images added, 0 not loaded\n",
      "24200 Images added, 0 not loaded\n",
      "Batch Loaded 120: 439 secs\n",
      "Remaining time 1252 mins\n",
      "25650 Images added, 0 not loaded\n",
      "25250 Images added, 0 not loaded\n",
      "25850 Images added, 0 not loaded\n",
      "25050 Images added, 0 not loaded\n",
      "25450 Images added, 0 not loaded\n",
      "25700 Images added, 0 not loaded\n",
      "25900 Images added, 0 not loaded\n",
      "25100 Images added, 0 not loaded\n",
      "25300 Images added, 0 not loaded\n",
      "25950 Images added, 0 not loaded\n",
      "25150 Images added, 0 not loaded\n",
      "25500 Images added, 0 not loaded\n",
      "25750 Images added, 0 not loaded\n",
      "26000 Images added, 0 not loaded\n",
      "Batch Loaded 129: 228 secs\n",
      "25200 Images added, 0 not loaded\n",
      "Batch Loaded 125: 245 secs\n",
      "25800 Images added, 0 not loaded\n",
      "Batch Loaded 128: 263 secs\n",
      "25550 Images added, 0 not loaded\n",
      "25350 Images added, 0 not loaded\n",
      "25600 Images added, 0 not loaded\n",
      "Batch Loaded 127: 384 secs\n",
      "25400 Images added, 0 not loaded\n",
      "Batch Loaded 126: 433 secs\n",
      "Remaining time 1233 mins\n",
      "26650 Images added, 0 not loaded\n",
      "26450 Images added, 0 not loaded\n",
      "26250 Images added, 0 not loaded\n",
      "26050 Images added, 0 not loaded\n",
      "26850 Images added, 0 not loaded\n",
      "26500 Images added, 0 not loaded\n",
      "26900 Images added, 0 not loaded\n",
      "26300 Images added, 0 not loaded\n",
      "26100 Images added, 0 not loaded\n",
      "26700 Images added, 0 not loaded\n"
     ]
    }
   ],
   "source": [
    "# Create Directories for the HD5 encoded batches\n",
    "!mkdir contents\n",
    "!mkdir contents\\\\train\n",
    "!mkdir contents\\\\validation\n",
    "!mkdir contents\\\\test\n",
    "\n",
    "# Data dictionaries\n",
    "train_url      = 'C:\\\\Users\\\\User\\\\.kaggle\\\\competitions\\\\imaterialist-challenge-furniture-2018\\\\train.json'\n",
    "test_url       = 'C:\\\\Users\\\\User\\\\.kaggle\\\\competitions\\\\imaterialist-challenge-furniture-2018\\\\test.json'\n",
    "validation_url = 'C:\\\\Users\\\\User\\\\.kaggle\\\\competitions\\\\imaterialist-challenge-furniture-2018\\\\validation.json'\n",
    "\n",
    "# Load the Training Batches\n",
    "load_dispatcher(train_url, \"train\")\n",
    "\n",
    "# Load the Validation Batches\n",
    "load_dispatcher(validation_url, \"validation\")\n",
    "\n",
    "# Load the Test Batches\n",
    "load_dispatcher(test_url, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "iMaterialist-download-dataset.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
