{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow - MNIST For ML Beginners \n",
    "\n",
    "This Code Along tutorial is based on TensorFlow.org's MNIST For ML Beginners tutorial. They describe this as the Hello World\n",
    "example for those just starting to use TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (Housekeeping)\n",
    "\n",
    "If you haven't already, you will need to install Tensorflow and Jupyter notebook, as well as Python. I recommend Python 3.\n",
    "\n",
    "For those on Windows 10, you should install Python 3.5 (not 3.6). The Google support blogs still show people continuing to have\n",
    "incompatibility problems with TensorFlow and Python 3.6.\n",
    "\n",
    "If you already have Python 3.6 loaded, try importing tensorflow first and see if the import is successfully, before deciding to downgrade to 3.5\n",
    "\n",
    "You should also be using pip version 9. If you are using an older version, you may need to upgrade.\n",
    "\n",
    "\n",
    "### Python 3.5\n",
    "\n",
    "You can download Python 3.5 here. Goto the bottom of the page and select the download for your OS.\n",
    "\n",
    "https://www.python.org/downloads/release/python-350/\n",
    "\n",
    "If you are not sure which version you have, do the following:\n",
    "\n",
    "C:> python --version\n",
    "\n",
    "<span style='color:red; font-weight:bold'>NOTE: If you have Python 2 and 3 installed, then wherever I have python on the command line, replace with python3</span>\n",
    "\n",
    "### PIP \n",
    "\n",
    "To upgrade to the latest verson of PIP (version 9 as of this writing), do the following (below is an example on Windows command prompt).\n",
    "\n",
    "C:> python -m pip install --upgrade pip \n",
    "\n",
    "If you are not sure which version you have, do the following:\n",
    "\n",
    "C:> pip --version\n",
    "\n",
    "### Tensorflow\n",
    "\n",
    "You want to install Tensorflow 1.6 for your platform. Your platform is a combination of your version of python (e.g., cp35) and your OS/Architecture (e.g., cp35m-win_amd64). \n",
    "\n",
    "You can find the wheels (.whl) install files for Tensorflow 1.6 here:\n",
    "\n",
    "https://pypi.python.org/pypi/tensorflow\n",
    "\n",
    "On my laptop (Win10), I am using:\n",
    "\n",
    "tensorflow-1.6.0rc1-cp35-cp35m-win_amd64.whl\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "You can install Juypter Notebook as follows:\n",
    "\n",
    "C:> python -m pip install jupyter\n",
    "\n",
    "\n",
    "### Putting it Together\n",
    "\n",
    "Let's see if everything is installed correctly:\n",
    "\n",
    "    1. Launch Jupyter Notebook\n",
    "    2. Create a Notebook\n",
    "    3. In an input cell, import Tensorflow\n",
    "    4. Run the cell.\n",
    "    \n",
    "#### Launch Jupyter Notebook\n",
    "\n",
    "From a command terminal, enter:\n",
    "\n",
    "C:> jupyter notebook\n",
    "\n",
    "This will launch a Jupyter notebook in a web browser (whatever is your default web browser). \n",
    "\n",
    "#### Create a Notebook\n",
    "\n",
    "Under the toolbar, select:\n",
    "\n",
    "File-> New Notebook -> Python 3\n",
    "\n",
    "A notebook will appear which will look similar to this.\n",
    "\n",
    "#### Import Tensorflow\n",
    "\n",
    "In the first input cell, enter the following and then select Run.\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "Python execution of the cell should complete without any errors. You may get deprecated warnings, just ignore them. Let me try it below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MNIST\n",
    "\n",
    "The MNIST dataset is a well-known dataset of images of handwritten digits (0 .. 9). It is used extensively as an example for\n",
    "training beginners in both Computer Vision and ML frameworks. \n",
    "\n",
    "Kaggle describes this dataset as:\n",
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "<img src='mnist.png'>\n",
    "\n",
    "Each image has a label, between 0 and 9 and represents a single digit. Each image is 28 pixels by 28 pixels. The pixels are black or white (0 or 1). Each image represents a total of 784 pixels (28 x 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data\n",
    "\n",
    "Tensorflow comes with several builtin datasets to get started. These are located in the package tensorflow.examples.tutorials.\n",
    "\n",
    "The dataset contains a total of 70,000 images, split up as follows:\n",
    "\n",
    "- 55,000 training data\n",
    "- 10,000 test data\n",
    "- 5,000 validation data\n",
    "\n",
    "Let's start by getting the data from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MNIST input_data function from the tutorials.mnist package\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Read in the data\n",
    "# The paramter one_hot=True refers to the label which is categorical (1-10). \n",
    "# The paramter causes the label to be re-encoded as a 10 column vector.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at what type of object 'mnist' is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it is a Tensorflow object of type Datasets. The training, test and validation data can be accessed\n",
    "as attributes. \n",
    "\n",
    "Let's look at their data types. We can see they are Tensorflow Dataset (non-plural) data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( type(mnist.train) )\n",
    "print( type(mnist.test) )\n",
    "print( type(mnist.validation) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the Dataset\n",
    "\n",
    "Let's now look closer to what's inside the Tensorflow dataset object for the MNIST dataset. \n",
    "\n",
    "The images and corresponding labels are accessed by the attributes images and labels, respectively. How convenient, such a logical name for an attribute! \n",
    "\n",
    "Both the images and labels are a Numpy multi-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist.train.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the training data has the expected 55,000 images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the length (number of images) of the list of images.\n",
    "print( len(mnist.train.images) )\n",
    "print( len(mnist.train.labels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the contents of one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.images[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>OMG</b>. According to the Tensorflow tutorial, these are B&W images should have only 0 and 1 values. But we\n",
    "see floating point values between 0 and 1. What's wrong!\n",
    "\n",
    "Well, the Google writer for this tutorial goofed. A correct explanation of what the format of the images are can be found on Yann Lecun's website:\n",
    "\n",
    "The original black and white (bilevel) images from MNIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  - http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting an Image\n",
    "\n",
    "Let's plot one of the images in the training set. To do so, we will use the plotting functions of the matplotlib package. Let's start by importing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This line is specific to python notebooks (not python). \n",
    "# It causes plots to automatically be rendered (displayed) without issuing a show command.\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now render (display) one of the images. \n",
    "\n",
    "<b>WAIT</b>, the images in the Tensorflow MNIST dataset are already flattened. That is, they are a single vector of 784 inputs. To display it, we need to reshape the vector back into a 28 x 28 pixel matrix. Then we can plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show that the shape of the image is already flatten (will output as 784)\n",
    "print( mnist.train.images[1].shape )\n",
    "\n",
    "# Let's now reshape it into a 28 x 28 matrix\n",
    "image = mnist.train.images[1].reshape(28,28)\n",
    "print( image.shape )\n",
    "\n",
    "# Let's plot it now\n",
    "plt.imshow( image )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Labels\n",
    "\n",
    "The corresponding labels are a 2D matrix. Each row is for the corresponding image (that is index 0 in labels is for image at index 0 in images).\n",
    "\n",
    "Each row has ten columns. The labels have been encoded as a one-hot encoding. In this case, if the image is a 3, then there is a 1 at index 3, and all the other columns are a zero.\n",
    "\n",
    "Let's look at the label vector for the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, there is a 1 at index 3 (fourth location, starting at 0)\n",
    "mnist.train.labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - Softmax Regression\n",
    "\n",
    "We are going to build a simple neural network. There will not be a convolutional layer. That is, we will use all 784 pixels as input. The type of neural network we will build is called a Softmax Regression.\n",
    "\n",
    "A Softmax Regression is mathematically represented as:\n",
    "\n",
    "                                               y = softmax( Wx + b )\n",
    "                                               \n",
    "y = the predicted value<br/>\n",
    "Wx = the weight for each input node (i.e., pixel in our case)<br/>\n",
    "b  = the bias for each output node\n",
    "\n",
    "In our Neural Network, we will have the following:\n",
    "\n",
    "    - An input layer of 784 inputs\n",
    "    - A softmax layer\n",
    "    - An output layer of 10 nodes\n",
    "\n",
    "INPUT LAYER => SOFTMAX => OUTPUT LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "We will use the softmax layer to make our predictions. Each output from softmax will be a number between 0 and 1, representing a percent. That is, if the output for the node 3 is 0.8, then this means 80% prediction. We will choose the output with the highest percent when making a prediction.\n",
    "\n",
    "Softmax is a mathematical function that takes a set of values, which may otherwise not add up to 1, and outputs a new set of numbers when totaled will add up to 1. That is, we use softmax() so that all our outputs for each image add up to 1 (100%).\n",
    "\n",
    "Softmax will be our 'activation' function between the input layer and output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "\n",
    "Let's build our neural network now. We will do the following:\n",
    "\n",
    "### Design\n",
    "\n",
    "    - Create the placeholder for the input data\n",
    "    - Design the layers\n",
    "    - Design the optimizer\n",
    "    \n",
    "### Run\n",
    "\n",
    "    - Run the training data through the neural network to produce (learn) a model\n",
    "    - Evaluate our model against the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Placeholder for the Input Data\n",
    "\n",
    "We start by creating a tensorflow placeholder for the input vector. A placholder is a tensor whose final shape is not defined until execution. In this case, we know that each image will have 784 inputs (pixels), but we don't know how many images we will feed through the neural network.\n",
    "\n",
    "So let's create a placeholder for a matrix where the number of rows (i.e., images) will be set at run time by setting the row\n",
    "parameter to None. Since we do know how many inputs there will be, we will set the column parameter to 784. We also know that\n",
    "our data is floating point values between 0 and 1, so we will set the data type to float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "\n",
    "Let's now design our input layer. We need two things: weights and biases. \n",
    "\n",
    "Each input feature (pixel) will need a weight (which our model will learn during training). The weight is multipled against the value of the input (pixel), which we symbolically represent as Wx. \n",
    "\n",
    "Each output from the layer will need a bias (which our model will learn during training). The bias is added to the result of the weight multipled by the pixel value (Wx).\n",
    "\n",
    "Let's create two tensorflow variables for our weights and biases. The weights (which we call W) will need to be a 2D matrix. The rows are the number of inputs, which is 784 and the columns the number of outputs, which is 10 (one for each digit).\n",
    "\n",
    "The bias will be a vector of size 10 (one for each digit).\n",
    "\n",
    "We need to initialize our weights and biases to some initial value. We could use anything, since the final values will be learned. For simplicity, we will initialize them to zeros (tf.zeros() method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weights for the input layer\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# The bias for the output from the input layer\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it together into an input layer. We will use the Tensorflow method tf.matmul() to do a matrix multiplication of the weights (our variable W) and the inputs (our placeholder x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Function\n",
    "\n",
    "Let's now design our softmax activation function. \n",
    "\n",
    "We use the tf.nn.softmax() method in Tensorflow. We will pass in our input layer as the argument to the softmax() method, which will squash the outputs to all add up to 1. \n",
    "\n",
    "The output y will our 10 element vector, one element per digit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the softmax activation function to the input layer\n",
    "y = tf.nn.softmax(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the Optimizer\n",
    "\n",
    "Let's now design the optimizer.\n",
    "\n",
    "First, we will need a placeholder for our predicted values. We know that that the output will be between 0 and 1 (floating point), and we will have 10 outputs (one per digit), but we don't know the number of images (samples) which will be ran through the neural network. We will use None in the rows parameter as the placeholder for the final number of images that are feed through the neural network during training.\n",
    "\n",
    "We will use the name y_ to be the tensor of our predicted values, where y is the tensor of our actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now design our cost function. Our cost function accumulates the loss between the predicted and actual value across a batch. For this demo, we are using the cross entropy formula. This is common to use in a softmax regression, where you have real value outputs (regression) but are actually doing a classification (output with the highest prediction).\n",
    "\n",
    "The implementation in Tensorflow of the cross entropy function is from Google's tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost function which accumulates the loss for a batch\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y, labels=y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design our optimizer. This is the method that adjusts the values of the weights and biases, based on minizing the cost value during training.\n",
    "\n",
    "We also need to set a learning rate. This is multiplied against the gradient calculation. It's used to prevent huge swings in setting weights which can result in either converging at a local (instead of global) optima, or not converging at all (infinite gradient).\n",
    "\n",
    "Wow, Google tutorial has it set to 0.5, that's big. You may want to play around with other learning rates, like 0.01 and 0.001, which are more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate for Gradient Descent algorithm\n",
    "learning_rate = 0.5\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Graph\n",
    "\n",
    "We've built our Tensorflow graph for training our data. So, let's start training it.\n",
    "\n",
    "First, we need to call Tensorflow's global_variables_initializer() method to initialize the variables we've defined. We will create this as another node, which will be the first node we run (evaluate) in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also a good idea to know how long your training takes, so let's import the time library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our hyperparameters.\n",
    "\n",
    "We need to set the number of epochs (that's how many times we run the training data through the neural network), and the batch size. The batch size is a small subset of the entire training set. We will be running a batch at a time per epoch. After each batch, then the cost is computed and backpropagated through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000      # run a 1000 epochs\n",
    "batch_size = 100   # for each epoch, train in batches of 100 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run the graph now!\n",
    "\n",
    "We start by creating a tensorflow session (tf.Session()). Within the session we can run (evaluate) parts of the graph we designed.\n",
    "\n",
    "We start by initializing the tensor variables we defined for the weights and biases.\n",
    "\n",
    "We then run our training data through our neural network for the number of epochs we defined. For each epoch, we get a randomly shuffled batch from the training data and feed the batch (i.e. feed dictionary) into the neural network by running (evaluate)\n",
    "the optimizer node in our graph.\n",
    "\n",
    "Once we've trained the model, then we create some new nodes to calculate accuracy and evaluate against the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # run our training data through the neural network for each epoch\n",
    "    for epoch in range(epochs):\n",
    "      # Get a batch (random shuffled) from the training data\n",
    "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "      \n",
    "      # Feed this batch through the neural network.\n",
    "      sess.run(optimizer, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "      \n",
    "      if epoch % 100 == 0:\n",
    "        print(\"Epoch: \", epoch)\n",
    "        \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Training Time:\", end - start)\n",
    "    \n",
    "    # Test the Model\n",
    "    \n",
    "    # Let's select the highest percent from the softmax output per image as the prediction.\n",
    "    prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    \n",
    "    # Let's create another node for calculating the accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    \n",
    "    # Now let's run our test images through the model to calculate our accuracy\n",
    "    print(\"Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate of Model\n",
    "\n",
    "The last three steps above is where our test data was ran through the model and produced how accurate our model was on the test data.\n",
    "\n",
    "The explanation in Google's tutorial I find awful! If your an expert in TF, then you'd already understand. But this tutorial is for beginners! So, let me explain it to you.\n",
    "\n",
    "After training the model, we created a node for prediction. This node compares two vectors, our predicted labels and our actual labels. Each vector is 10 elements long with a 1 in the predicted/actual digit location. So we are comparing the vectors. If they match (prediction matches actual), then we have a TRUE; otherwise a FALSE. That's how we are going to get our accuracy percentage calculated.\n",
    "\n",
    "Next, we create the node accuracy. This node is a cost function!\n",
    "\n",
    "We then run the accuracy node, feeding it the test images as the x variable and the test labels as the y_ variable. This will result in the test images being ran through the model (which is in memory) and the corresponding output vectors evaluated against the actual labels of the test images (y_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Have Now Completed TensorFlow's Equivalent of a Hello World Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
